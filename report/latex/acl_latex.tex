\documentclass[11pt]{article}

% Use review mode for submission, change to final for camera-ready
\usepackage[preprint]{acl}

% Standard packages
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

% Additional packages
\usepackage{graphicx}
\usepackage{booktabs}

% Packages for methodology section (TikZ diagrams, algorithms, code listings)
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}
\IfFileExists{multirow.sty}{\usepackage{multirow}}{}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{bookmark}

% Title and authors
\title{Architectures for Code Development with LLMs:\\ A Comparative Study of Multi-Agent Approaches}

\author{
  Riccardo Marconi \\
  Politecnico di Torino \\
  \small\texttt{riccardo.marconi@studenti.polito.it}
  \And
  Dorotea Monaco \\
  Politecnico di Torino \\
  \small\texttt{s349653@studenti.polito.it}
  \And
  Luigi Gonnella \\
  Politecnico di Torino \\
  \small\texttt{s341988@studenti.polito.it}
  \AND
  Name Surname \\
  Politecnico di Torino \\
  \small\texttt{email@studenti.polito.it}
  \And
  Name Surname \\
  Politecnico di Torino \\
  \small\texttt{email@studenti.polito.it}
}

\begin{document}
\maketitle

\begin{abstract}
  Large language models (LLMs) demonstrate impressive code generation capabilities, yet single-prompt interactions often fail on complex development tasks. We investigate whether multi-agent architectures improve code quality through role specialization and iterative refinement. We compare three approaches---Naive (one-shot generation), Single-Agent (5-stage pipeline with self-refinement), and Multi-Agent (Planner-Coder-Critic coordination)---across 25 programming tasks spanning five domains and three difficulty levels. Surprisingly, Multi-Agent and Naive achieve identical overall correctness (77.6\%), outperforming Single-Agent (74.6\%). However, Multi-Agent excels in algorithmically demanding domains (DSA: 85.9\%, Logic: 75.7\%) and medium-difficulty tasks (91.8\%), justifying its 20$\times$ computational overhead only for specific task types. Our findings reveal that architectural sophistication benefits complex algorithmic reasoning but can hurt performance on simple tasks through over-refinement.
\end{abstract}

\section{Introduction}

Large language models have revolutionized automated code generation, with models like Codex~\citep{chen2021evaluating} and Code Llama~\citep{roziere2023code} demonstrating remarkable capabilities on programming benchmarks. However, single-prompt approaches often struggle with complex development tasks that require multi-step reasoning, systematic debugging, and quality assurance. Real-world software development involves iterative refinement cycles---planning, implementation, testing, and revision---that single interactions cannot capture.

Recent work in multi-agent systems~\citep{hong2023metagpt, qian2023chatdev} suggests that distributing responsibilities across specialized agents (e.g., planning, coding, reviewing) may improve software development outcomes by mimicking human team workflows. Yet, a fundamental question remains unanswered: does the computational overhead of multi-agent coordination consistently translate to higher code quality? No systematic evaluation exists comparing single-agent and multi-agent architectures for code generation across diverse task types and complexity levels, making it unclear when architectural sophistication is justified.

This work addresses this gap through a controlled empirical study comparing three architectural approaches across 25 programming tasks spanning five domains (data structures, logic, mathematics, string processing, list manipulation) and three difficulty levels. We evaluate a Naive baseline (one-shot generation), a Single-Agent pipeline with five iterative refinement stages, and a Multi-Agent system with role-specialized coordination (Planner-Coder-Critic). Our findings challenge the assumption that architectural sophistication uniformly improves performance, revealing that the benefits of multi-agent coordination are highly task-dependent and come at significant computational cost.

\subsection{Research Questions}

We address the following research questions from the assignment requirements:

\begin{enumerate}
  \item \textbf{RQ1:} How do the architectures compare in terms of functional correctness and code quality?
  \item \textbf{RQ2:} How do agent coordination strategies impact correctness?
  \item \textbf{RQ3:} Does modular role separation improve code generation?
\end{enumerate}

\section{Background}

\textbf{LLM-based Code Generation.} Neural code generation has progressed from early transformer models~\citep{austin2021program} to specialized systems like Codex~\citep{chen2021evaluating}, StarCoder~\citep{li2023starcoderbase}, Code Llama~\citep{roziere2023code}, and Qwen2.5-Coder~\citep{qwen2024}, achieving increasingly strong performance on benchmarks like HumanEval. 

\textbf{Multi-Agent Systems.} ChatDev~\citep{qian2023chatdev} and MetaGPT~\citep{hong2023metagpt} apply role-based agent collaboration to software development, with specialized agents handling requirements, design, implementation, and testing. These systems demonstrate capabilities for high-level engineering workflows but focus on system design rather than low-level algorithmic correctness.

\textbf{Iterative Refinement.} Chain-of-thought prompting~\citep{wei2022chain} and self-debugging~\citep{chen2023program} show that iterative refinement can improve LLM outputs. However, recent work~\citep{huang2023large} reveals that self-refinement sometimes degrades solution quality without external feedback, raising questions about when iteration helps versus hurts---a gap our comparative evaluation addresses.

Our work provides a systematic comparison of architectural paradigms (single-shot, iterative pipeline, multi-agent coordination) using controlled conditions to isolate the impact of architectural decisions on code correctness across task types and difficulty levels.


% methodology section (imported from external file)
\input{methodology}

% experiments section (imported from external file)
\input{experiments}

% conclusion section (imported from external file)
\input{conclusion}

\bibliography{custom}

\end{document}
