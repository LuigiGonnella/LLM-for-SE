\documentclass[11pt]{article}

% Use review mode for submission, change to final for camera-ready
\usepackage[preprint]{acl}

% Standard packages
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

% Additional packages
\usepackage{graphicx}
\usepackage{booktabs}

% Packages for methodology section (TikZ diagrams, algorithms, code listings)
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}
\IfFileExists{multirow.sty}{\usepackage{multirow}}{}
\usepackage{array}
\usepackage{url}
\usepackage{hyperref}
\usepackage{bookmark}

% Title and authors
\title{Architectures for Code Development with LLMs:\\ A Comparative Study of Multi-Agent Approaches}

\author{
  Riccardo Marconi \\
  Politecnico di Torino \\
  \small\texttt{riccardo.marconi@studenti.polito.it}
  \And
  Name Surname \\
  Politecnico di Torino \\
  \small\texttt{email@studenti.polito.it}
  \And
  Name Surname \\
  Politecnico di Torino \\
  \small\texttt{email@studenti.polito.it}
  \AND
  Name Surname \\
  Politecnico di Torino \\
  \small\texttt{email@studenti.polito.it}
  \And
  Name Surname \\
  Politecnico di Torino \\
  \small\texttt{email@studenti.polito.it}
}

\begin{document}
\maketitle

\begin{abstract}
  Large language models (LLMs) demonstrate impressive code generation capabilities, yet single-prompt interactions often fail on complex development tasks. We investigate whether multi-agent architectures improve code quality through role specialization and iterative refinement. We compare three approaches---Naive (one-shot generation), Single-Agent (5-stage pipeline with self-refinement), and Multi-Agent (Planner-Coder-Critic coordination)---across 25 programming tasks spanning five domains and three difficulty levels. Surprisingly, Multi-Agent and Naive achieve identical overall correctness (77.6\%), outperforming Single-Agent (74.6\%). However, Multi-Agent excels in algorithmically demanding domains (DSA: 85.9\%, Logic: 75.7\%) and medium-difficulty tasks (91.8\%), justifying its 20$\times$ computational overhead only for specific task types. Our findings reveal that architectural sophistication benefits complex algorithmic reasoning but can hurt performance on simple tasks through over-refinement.
\end{abstract}

\section{Introduction}

% TODO: Write introduction section
\textit{Large language models have revolutionized automated code generation, with models like Codex~\citep{chen2021evaluating} and Code Llama~\citep{roziere2023code} demonstrating remarkable capabilities. However, single-prompt approaches often struggle with complex development tasks requiring multi-step reasoning, systematic debugging, and quality assurance.}

\textit{Recent work in multi-agent systems~\citep{hong2023metagpt, qian2023chatdev} suggests that distributing responsibilities across specialized agents (e.g., planning, coding, reviewing) may improve software development outcomes. Yet, no systematic evaluation exists comparing single-agent and multi-agent architectures for code generation across diverse task types and complexity levels.}

\textit{This work addresses this gap through a controlled comparison of three architectural approaches on 25 programming tasks. Our key finding is that architectural sophistication does not uniformly improve performance---benefits are highly task-dependent, with multi-agent coordination excelling on algorithmic reasoning but struggling on simple pattern-matching tasks.}

\subsection{Research Questions}

We address the following research questions from the assignment requirements:

\begin{enumerate}
  \item \textbf{RQ1:} How do the architectures compare in terms of functional correctness and code quality?
  \item \textbf{RQ2:} How do agent coordination strategies impact correctness?
  \item \textbf{RQ3:} Does modular role separation improve code generation?
\end{enumerate}

\section{Background}

% TODO: Expand with more related work
\textit{Provide an overview of relevant work in the literature related to your task.}

\textbf{LLM-based Code Generation.} Early work on neural code generation~\citep{austin2021program} demonstrated feasibility of program synthesis from natural language. Recent code-specialized models like StarCoder~\citep{li2023starcoderbase}, CodeLlama~\citep{roziere2023code}, and Qwen2.5-Coder~\citep{qwen2024} achieve strong performance on benchmarks like HumanEval~\citep{chen2021evaluating}.

\textbf{Multi-Agent Systems.} ChatDev~\citep{qian2023chatdev} and MetaGPT~\citep{hong2023metagpt} demonstrate that role-based agent collaboration can improve software development workflows. However, these systems focus on high-level design rather than low-level code correctness.

\textbf{Self-Refinement.} Chain-of-thought reasoning~\citep{wei2022chain} and self-debugging~\citep{chen2023program} show that iterative refinement can improve LLM outputs. Our work systematically compares architectures with and without refinement mechanisms.

% methodology section (imported from external file)
\input{methodology}

% experiments section (imported from external file)
\input{experiments}

% conclusion section (imported from external file)
\input{conclusion}

\bibliography{custom}

\end{document}
