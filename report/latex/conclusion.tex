\section{Conclusion}
We presented a systematic comparison of three architectural approaches for LLM-based code generation, across 25 programming tasks.

The Naive and Multi-Agent architectures achieved identical overall functional correctness (77.6\%), yet their performance profiles differ substantially.
The Multi-Agent system justifies its computational costs in reasoning-intensive domains (Logic +10.8pp, DSA +9.4pp) where systematic planning yields a clear advantage.
In contrast, for simple pattern-matching, agentic coordination introduces unnecessary overhead that degrades performance compared to the Naive baseline. \\
Although Naive unexpectedly outperforms on Hard tasks, likely avoiding the over-refinement pitfalls of complex pipelines, our code quality analysis reveals that the Multi-Agent
approach produces more modular solutions, preserving maintainability despite increased solution complexity.

These findings suggest adaptive architecture selection: Naive for rapid prototyping, string manipulation, and known hard problems; Single-Agent for general-purpose development with
systematic edge case checking; Multi-Agent for algorithmic reasoning, constraint satisfaction, and production-quality requirements in complex domains. \\

\textbf{Limitations}: Our study is limited to 25 tasks and three model families. Future work should evaluate on larger benchmarks (APPS, CodeContests), investigate optimal refinement iteration counts,
explore heterogeneous multi-agent configurations, and develop adaptive routing mechanisms to select architectures based on task characteristics.